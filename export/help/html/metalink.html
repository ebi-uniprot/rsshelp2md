<div>
 <p>
  <strong>
   FTP downloads
  </strong>
 </p>
 <p>
  Every folder on our
  <a href="https://ftp.uniprot.org/">
   FTP server
  </a>
  contains a file called RELEASE.metalink that specifies the size and MD5 checksum of every file in that folder, e.g.
  <br/>
  <a href="https://ftp.uniprot.org/pub/databases/uniprot/knowledgebase/RELEASE.metalink">
   https://ftp.uniprot.org/pub/databases/uniprot/knowledgebase/RELEASE.metalink
  </a>
 </p>
 <p>
  <a href="http://en.wikipedia.org/wiki/Metalink">
   Metalink
  </a>
  is an extensible metadata file format that describes one or more computer files available for download. It facilitates file verification and recovery from data corruption and lists alternate download sources (mirror URIs).
 </p>
 <p>
  Various command line download tools, e.g.
  <a href="http://curl.haxx.se/">
   cURL
  </a>
  version 7.30 or higher and
  <a href="http://aria2.sourceforge.net/">
   aria2
  </a>
  , support metalink.
 </p>
 <p>
  Example: The following command will download all files in the
  <code>
   current_release/
  </code>
  folder and verify their MD5 checksums:
 </p>
 <pre>curl --metalink https://ftp.uniprot.org/pub/databases/uniprot/current_release/RELEASE.metalink</pre>
 <p>
  They will be downloaded from one of the alternative locations mentioned in the metalink file. If one FTP server goes down during a download, programs can automatically switch to another mirror location. Some programs can also download segments from several FTP locations at the same time, which can make downloads much faster.
 </p>
 <p>
  Please note that UniProt can be downloaded from the consortium member FTP sites at three different geographical locations:
 </p>
 <p>
  USA:
  <a href="https://ftp.uniprot.org/pub/databases/uniprot">
   https://ftp.uniprot.org/pub/databases/uniprot
  </a>
  <br/>
  UK:
  <a href="ftp://ftp.ebi.ac.uk/pub/databases/uniprot">
   ftp://ftp.ebi.ac.uk/pub/databases/uniprot
  </a>
  <br/>
  Switzerland:
  <a href="https://ftp.expasy.org/databases/uniprot">
   https://ftp.expasy.org/databases/uniprot
  </a>
 </p>
 <p>
  <strong>
   HTTP downloads
  </strong>
 </p>
 <p>
  Due to HTTP transport unreliability (HTTP streams tend to fail after a while due to packet loss), large downloads should be split into smaller chunks using the "offset" and "limit" functions. These are described in our API help page
  <a href="http://www.uniprot.org/help/api%5Fqueries">
   Retrieving entries via queries
  </a>
  .
 </p>
 <p>
  1) Start by retrieving the number of results in your query by checking the "X-Total-Results" header like in the example
  <a href="http://www.uniprot.org/help/programmatic%5Faccess#downloading">
   Download all UniProt sequences for a given organism in FASTA format
  </a>
  .
 </p>
 <p>
  2) If the number of results x is greater than 50000, repeat your query and append the following to the URL:
 </p>
 <pre>&amp;offset=0&amp;limit=50000
&amp;offset=50000&amp;limit=50000
&amp;offset=100000&amp;limit=50000 etc.
</pre>
 <p>
  Also use
  <code>
   compress=yes
  </code>
 </p>
 <p>
  e.g. (using 50 instead of 50000 to make the file more manageable in the browser)
  <pre>https://www.uniprot.org/uniprot/?query=organism:%22Homo%20sapiens%20(Human)%20%5B9606%5D%22&amp;fil=&amp;offset=0&amp;limit=50&amp;compress=yes&amp;format=fasta
https://www.uniprot.org/uniprot/?query=organism:%22Homo%20sapiens%20(Human)%20%5B9606%5D%22&amp;fil=&amp;offset=50&amp;limit=50&amp;compress=yes&amp;format=fasta
https://www.uniprot.org/uniprot/?query=organism:%22Homo%20sapiens%20(Human)%20%5B9606%5D%22&amp;fil=&amp;offset=100&amp;limit=50&amp;compress=yes&amp;format=fasta
</pre>
 </p>
 <p>
  etc.
 </p>
 <p>
  3) Once you have your download, use
  <code>
   gzip -t
  </code>
  to check the integrity of your file. Uncompress the chunks and concatenate them into a single download file.
 </p>
</div>